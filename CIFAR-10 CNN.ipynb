{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CIFAR-10 Dataset\n",
    "CIFAR-10 consists of 60000 32x32 color images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. \n",
    "\n",
    "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. \n",
    "\n",
    "The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niall\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50000, 32, 32, 3)\n",
      "Y_train shape: (50000, 10)\n",
      "X_test shape: (10000, 32, 32, 3)\n",
      "Y_test shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# convert class vectors to binary vectors\n",
    "Y_train = np_utils.to_categorical(y_train)\n",
    "Y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('Y_test shape:', Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "A 10000x3072 numpy array of uint8s. Each row of the array stores a 32x32 colour image. The first 1024 entries contain the red channel values, the next 1024 the green, and the final 1024 the blue. The image is stored in row-major order, so that the first 32 entries of the array are the red channel values of the first row of the image.\n",
    "\n",
    "### Labels\n",
    "A list of 10000 numbers in the range 0-9. The number at index i indicates the label of the ith image in the array data.\n",
    "\n",
    "### Label Names\n",
    "The dataset contains another file, called batches.meta. It too contains a Python dictionary object. It has the following entries:\n",
    "A 10-element list which gives meaningful names to the numeric labels in the labels array described above. For example, label_names[0] == \"airplane\", label_names[1] == \"automobile\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50000, 32, 32, 3)\n",
      "X_test shape: (10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# normalize inputs from 0-255 to 0.0-1.0\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 600x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot a chosen image based on index\n",
    "img = 100\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(X_train[img], interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definitions for training\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers import Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "\n",
    "img_size = 32\n",
    "img_channels = 3\n",
    "nb_classes = 10\n",
    "batch_size = 128\n",
    "epoch_max = 15\n",
    "drop_rate = 0.2\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BatchNormalization\n",
    "Normalize the activations of the previous layer at each batch, i.e. applies a transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niall\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(32, 32, 3..., activation=\"relu\", padding=\"same\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\niall\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(2, 2), padding=\"same\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape of last convolution layers: (None, 4, 4, 32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niall\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization()`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 32)          9248      \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 32)          9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 4, 4, 32)          9248      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 590,922\n",
      "Trainable params: 588,874\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Adapted from: http://www.cs.nthu.edu.tw/~shwu/courses/ml/labs/11_NN_Regularization/11_NN_Regularization.html\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "\n",
    "model_cnn = Sequential()\n",
    "\n",
    "# convolutional hidden layers\n",
    "for i in range(7):\n",
    "    model_cnn.add(Convolution2D(32, 3, 3, \n",
    "                        input_shape=(img_size, img_size, img_channels), \n",
    "                        border_mode='same', activation='relu'))\n",
    "    \n",
    "    if (i + 1) % 2 == 0:\n",
    "        model_cnn.add(MaxPooling2D(pool_size=(2, 2), border_mode='same'))\n",
    "model_cnn.add(Dropout(drop_rate))\n",
    "    \n",
    "print('Output shape of last convolution layers: {0}'.format(model_cnn.output_shape))\n",
    "model_cnn.add(Flatten())\n",
    "\n",
    "# fully connected hidden layers\n",
    "for i in range(2):\n",
    "    model_cnn.add(Dense(512))\n",
    "    model_cnn.add(BatchNormalization(mode=0))\n",
    "    model_cnn.add(Activation('relu'))\n",
    "model_cnn.add(Dropout(drop_rate))\n",
    "\n",
    "# output layer\n",
    "model_cnn.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "# compile model\n",
    "model_cnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model summary\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 314s 6ms/step - loss: 1.5819 - acc: 0.4299\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 310s 6ms/step - loss: 1.1493 - acc: 0.5888\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 305s 6ms/step - loss: 0.9812 - acc: 0.6517\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 318s 6ms/step - loss: 0.8710 - acc: 0.6907\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 310s 6ms/step - loss: 0.8003 - acc: 0.7185\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 303s 6ms/step - loss: 0.7437 - acc: 0.7397 5s -\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 302s 6ms/step - loss: 0.6944 - acc: 0.7584\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 315s 6ms/step - loss: 0.6550 - acc: 0.7700\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 313s 6ms/step - loss: 0.6154 - acc: 0.7819\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 305s 6ms/step - loss: 0.5880 - acc: 0.7946\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 313s 6ms/step - loss: 0.5517 - acc: 0.8070 4s - lo\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 303s 6ms/step - loss: 0.5284 - acc: 0.8134\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 309s 6ms/step - loss: 0.5030 - acc: 0.8237\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 311s 6ms/step - loss: 0.4802 - acc: 0.8308\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 301s 6ms/step - loss: 0.4560 - acc: 0.8390\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 316s 6ms/step - loss: 0.4346 - acc: 0.8437\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 310s 6ms/step - loss: 0.4214 - acc: 0.8489\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 298s 6ms/step - loss: 0.3963 - acc: 0.8601 4s - lo\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 312s 6ms/step - loss: 0.3848 - acc: 0.8625\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 303s 6ms/step - loss: 0.3676 - acc: 0.8682 3s - loss: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c32f5121d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "model_cnn.fit(x=X_train,y=Y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# Save weights in model to  avoid retraining\n",
    "model_cnn.save(\"CIFAR_model_cnn.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# Load saved model\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('CIFAR_model_cnn.h5')\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Error Percentage: 21.62%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate unsaved model\n",
    "scores = model_cnn.evaluate(X_test, Y_test, verbose = 0)\n",
    "print(\"Model Error Percentage: %.2f%%\" %(100 - scores[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Model Error Percentage: 21.62%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate loaded model on test data\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Loaded Model Error Percentage: %.2f%%\" %(100 - scores[1] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
